# -*- coding: utf-8 -*-
"""dihedral_GRU_LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ed_R-obKcYp5j5MCQMBW4JWBVRCn4An2
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
import tensorflow as tf
import numpy as np
import pandas as pd
import os
from sklearn.preprocessing import MinMaxScaler
# from tf.keras.models import Sequential  # This does not work!
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Input, Dense, GRU, LSTM, Embedding
from tensorflow.python.keras.optimizers import RMSprop
from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau

df_0 = pd.read_csv('output_1.csv', header=0)
df_0.drop(('TimeStep'), axis=1, inplace=True)   # drop the first column (time_step)

pwd

#df_0.loc[df_0['x1'] < 145,['x1','x2']] to find values bellow 145 over x1 and x2

target = ['x100', 'y100', 'z100']
shift_steps = 4         # number of steps we want to predict in the future; 5 is the best!
df_targets = df_0[target].shift(-shift_steps)

x_data = df_0.values[0:-shift_steps]
y_data = df_targets.values[:-shift_steps]   # targets

num_data = len(x_data)  # data-points
train_split = 0.9       # fraction of the data-set for the training-set ???? 0.9 is the best!

num_train = int(train_split * num_data)
num_test = num_data - num_train

x_train = x_data[0:num_train]
x_test = x_data[num_train:]

y_train = y_data[0:num_train]
y_test = y_data[num_train:]

num_x_signals = x_data.shape[1]
num_y_signals = y_data.shape[1]

# neural network works best on values roughly between -1 and 1
x_scaler = MinMaxScaler()
x_train_scaled = x_scaler.fit_transform(x_train)
x_test_scaled = x_scaler.transform(x_test)

y_scaler = MinMaxScaler()
y_train_scaled = y_scaler.fit_transform(y_train)
y_test_scaled = y_scaler.transform(y_test)

validation_data = (np.expand_dims(x_test_scaled, axis=0),
                   np.expand_dims(y_test_scaled, axis=0))

# Instead of training the RNN on the complete sequences of 100 observations, -
# we will use the following function to create a batch of shorter sub-sequences -
# picked at random from the training-data.

def batch_generator(batch_size, sequence_length):
    """
    Generator function for creating random batches of training-data.
    """

    # Infinite loop.
    while True:
        # Allocate a new array for the batch of input-signals.
        x_shape = (batch_size, sequence_length, num_x_signals)
        x_batch = np.zeros(shape=x_shape, dtype=np.float64)

        # Allocate a new array for the batch of output-signals.
        y_shape = (batch_size, sequence_length, num_y_signals)
        y_batch = np.zeros(shape=y_shape, dtype=np.float64)

        # Fill the batch with random sequences of data.
        for i in range(batch_size):
            # Get a random start-index.
            # This points somewhere into the training-data.
            idx = np.random.randint(num_train - sequence_length)
            
            # Copy the sequences of data starting at this index.
            x_batch[i] = x_train_scaled[idx:idx+sequence_length]
            y_batch[i] = y_train_scaled[idx:idx+sequence_length]
        
        yield (x_batch, y_batch)

batch_size = 128  #???? 128 is the best! Power of two thing!
sequence_length = 30    #???? 20 is the best for 4 shifts!
generator = batch_generator(batch_size=batch_size, sequence_length=sequence_length)
x_batch, y_batch = next(generator)

# Create the Recurrent Neural Network
model = Sequential()
model.add(GRU(units=512,    #????
              return_sequences=True,
              input_shape=(None, num_x_signals,)))

#model.add(LSTM(units=512,    #????
#              return_sequences=True,
#              input_shape=(None, num_x_signals,)))

# To predict 3 output-signals, we add a fully-connected (or dense) layer,
# which maps 512 values down to only 3 values.
model.add(Dense(num_y_signals, activation='sigmoid'))

if False:
    from tensorflow.python.keras.initializers import RandomUniform

    # Maybe use lower init-ranges.
    init = RandomUniform(minval=-0.02, maxval=0.02)  #????

    model.add(Dense(num_y_signals,
                    activation='linear',
                    kernel_initializer=init))

# Loss Function: MSE
warmup_steps = 0     #????

def loss_mse_warmup(y_true, y_pred):
    """
    Calculate the Mean Squared Error between y_true and y_pred,
    but ignore the beginning "warmup" part of the sequences.
    
    y_true is the desired output.
    y_pred is the model's output.
    """

    # The shape of both input tensors are:
    # [batch_size, sequence_length, num_y_signals].

    # Ignore the "warmup" parts of the sequences
    # by taking slices of the tensors.
    y_true_slice = y_true[:, warmup_steps:, :]
    y_pred_slice = y_pred[:, warmup_steps:, :]

    # These sliced tensors both have this shape:
    # [batch_size, sequence_length - warmup_steps, num_y_signals]

    # Calculate the MSE loss for each value in these tensors.
    # This outputs a 3-rank tensor of the same shape.
    loss = tf.losses.mean_squared_error(labels=y_true_slice,
                                        predictions=y_pred_slice)

    # Keras may reduce this across the first axis (the batch)
    # but the semantics are unclear, so to be sure we use
    # the loss across the entire tensor, we reduce it to a
    # single scalar with the mean function.
    loss_mean = tf.reduce_mean(loss)

    return loss_mean

# Compile Model
optimizer = RMSprop(lr=1e-3)   #???? 1e-3 to 1e-4 the best!
model.compile(loss=loss_mse_warmup, optimizer=optimizer, metrics=['accuracy'])
model.summary()

# Callback Functions
path_checkpoint = '23_checkpoint.keras'
callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,
                                      monitor='val_loss',
                                      verbose=1,
                                      save_weights_only=True,
                                      save_best_only=True)

callback_early_stopping = EarlyStopping(monitor='val_loss',
                                        patience=5, verbose=1)

callback_tensorboard = TensorBoard(log_dir='./23_logs/',
                                   histogram_freq=0,   #????
                                   write_graph=False)   #????
# Min learning rate 1e-4
callback_reduce_lr = ReduceLROnPlateau(monitor='val_loss',
                                       factor=0.1,    #???? 0.1
                                       min_lr=1e-4,   #????
                                       patience=0,
                                       verbose=1)

callbacks = [callback_early_stopping,
             callback_checkpoint,
             callback_tensorboard,
             callback_reduce_lr]

# Train the Recurrent Neural Network
model.fit_generator(generator=generator,
                    epochs=30,             #???? 20- keep it higher just for safty...
                    steps_per_epoch=150,   #???? 100 is the best for 4 shift and 200 best for 5 shifts!
                    validation_data=validation_data,
                    callbacks=callbacks)

# Load Checkpoint
try:
    model.load_weights(path_checkpoint)
except Exception as error:
    print("Error trying to load checkpoint.")
    print(error)
    
# Performance on Test-Set
result = model.evaluate(x=np.expand_dims(x_test_scaled, axis=0),
                        y=np.expand_dims(y_test_scaled, axis=0))

print("loss (test-set):", result)

# all xyz100 and sequence_length=10
print("0.241 on batch_size=128, steps_per_epoch=200, shift_steps=8 , and 0.9 split")
print("0.026 on batch_size=128, steps_per_epoch=200, shift_steps=3 , and 0.9 split")
print("0.226 on batch_size=128, steps_per_epoch=200, shift_steps=3 , and 0.8 split")
print("0.121 on batch_size=128, steps_per_epoch=200, shift_steps=3 , and 0.85 split")
print("0.011 on batch_size=128, steps_per_epoch=200, shift_steps=3 , and 0.95 split")
print("0.030 on batch_size=128, steps_per_epoch=200, shift_steps=5 , and 0.9 split") 
print("0.033 on batch_size=128, steps_per_epoch=100, shift_steps=4-5 , and 0.9 split") ###
#linear and softmax activations both suck!
#100 steps_per_epoch is the best for 4 shift and 200 best for 5 shifts! 
# >>>4*150 the best 

# all xyz100 and sequence_length=20 -----> the best
print("0.018 on batch_size=128, steps_per_epoch=200, shift_steps=3 , and 0.9 split")
print("0.008 on batch_size=128, steps_per_epoch=200, shift_steps=4 , and 0.9 split") ###
print("0.040 on batch_size=128, steps_per_epoch=200, shift_steps=5 , and 0.9 split")

# all xyz100 and sequence_length=40  getting worse!
print("0.028 on batch_size=128, steps_per_epoch=200, shift_steps=4 , and 0.9 split")

# all xyz100 and sequence_length=5   the lower the better!
print("0.009 on batch_size=128, steps_per_epoch=200, shift_steps=4 , and 0.9 split")
print("0.031 on batch_size=128, steps_per_epoch=200, shift_steps=5 , and 0.9 split")
print("0.132 on batch_size=128, steps_per_epoch=200, shift_steps=6 , and 0.9 split")

# all xyz100 and sequence_length=1   ?
print("0.045 on batch_size=128, steps_per_epoch=200, shift_steps=5 , and 0.9 split")
print("0.042 on batch_size=128, steps_per_epoch=500 got worse, shift_steps=5 , and 0.9 split")
print("0.014 on batch_size=128, epochs=30-60, steps_per_epoch=100 and 200, shift_steps=5 , and 0.9 split")

# Generate Predictions
def plot_comparison(start_idx, length=70, train=True):
    """
    Plot the predicted and true output-signals.
    
    :param start_idx: Start-index for the time-series.
    :param length: Sequence-length to process and plot.
    :param train: Boolean whether to use training- or test-set.
    """
    
    if train:
        # Use training-data.
        x = x_train_scaled
        y_true = y_train
    else:
        # Use test-data.
        x = x_test_scaled
        y_true = y_test
    
    # End-index for the sequences.
    end_idx = start_idx + length
    
    # Select the sequences from the given start-index and
    # of the given length.
    x = x[start_idx:end_idx]
    y_true = y_true[start_idx:end_idx]
    
    # Input-signals for the model.
    x = np.expand_dims(x, axis=0)

    # Use the model to predict the output-signals.
    y_pred = model.predict(x)
    
    # The output of the model is between 0 and 1.
    # Do an inverse map to get it back to the scale
    # of the original data-set.
    y_pred_rescaled = y_scaler.inverse_transform(y_pred[0])
    
    # For each output-signal.
    for signal in range(len(target)):
        # Get the output-signal predicted by the model.
        signal_pred = y_pred_rescaled[:, signal]
        
        # Get the true output-signal from the data-set.
        signal_true = y_true[:, signal]

        # Make the plotting-canvas bigger.
        plt.figure(figsize=(10,5))
        
        # Plot and compare the two signals.
        plt.plot(signal_true, label='true')
        plt.plot(signal_pred, label='pred')
        
        # Plot grey box for warmup-period.
        p = plt.axvspan(0, warmup_steps, facecolor='black', alpha=0.3)
        
        # Plot labels etc.
        plt.ylabel(target[signal])
        plt.legend()
        plt.show()

# Plot
plot_comparison(start_idx=0, length=80, train=False)