# -*- coding: utf-8 -*-
"""GRU_trj_path.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RCCWu3LNui-laSyECulgBxU6iAT-JEAz
"""

# !pip install tf-nightly-2.0-preview

# !ls -al

# !tf_upgrade_v2 --infile gru_trj_path.py --outfile gru_trj_path-upgraded.py

# !python gru_trj_path-upgraded.py

import matplotlib.pyplot as plt
import tensorflow as tf
import numpy as np
import pandas as pd
import os
import seaborn as sns
import glob
from scipy import stats
from statsmodels.tsa import stattools
from statsmodels.tsa.stattools import acf, pacf
from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import MaxAbsScaler
from sklearn.preprocessing import PowerTransformer
from sklearn.preprocessing import QuantileTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import Normalizer

from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Dense, GRU, LSTM, Embedding
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau

df = pd.read_csv('input_trj.csv', header=0)
df.head()

# Raw data
# Coordinates of atom 100 will be selected as target!
ax = plt.gca()
df.plot(kind='line', y='x100', ax=ax)
df.plot(kind='line', y='y100', color='red', ax=ax)
df.plot(kind='line', y='z100', color='green', ax=ax)

plt.show()

# Data generation: Increase dataset size from 1e3 to 10e4

import numpy as np
from random import randrange, choice
from sklearn.neighbors import NearestNeighbors

def SMOTE(T, N, k):
  """
  Returns (N/100) * n_minority_samples synthetic minority samples.

  Parameters
  ----------
  T : array-like, shape = [n_minority_samples, n_features]
      Holds the minority samples
  N : percetange of new synthetic samples: 
      n_synthetic_samples = N/100 * n_minority_samples. Can be < 100.
  k : int. Number of nearest neighbours. 

  Returns
  -------
  S : array, shape = [(N/100) * n_minority_samples, n_features]
  """    
  n_minority_samples, n_features = T.shape

  if (N % 100) != 0:
      raise ValueError("N must be < 100 or multiple of 100")

  N = int(N/100)
  n_synthetic_samples = int(N * n_minority_samples)
  S = np.zeros(shape=(n_synthetic_samples, n_features))

  #Learn nearest neighbours
  neigh = NearestNeighbors(n_neighbors = k)
  neigh.fit(T)

  #Calculate synthetic samples
  for i in range(n_minority_samples):
      nn = neigh.kneighbors(T[i].reshape(1, -1), return_distance=False)
      for n in range(N):
          nn_index = choice(nn[0])
          #NOTE: nn includes T[i], we don't want to select it 
          while nn_index == i:
              nn_index = choice(nn[0])

          dif = T[nn_index] - T[i]
          gap = np.random.random()
          S[n + i * N, :] = T[i,:] + gap * dif[:]

  return S

df_modified = df.iloc[:,1:] 
col = list(df_modified.columns) 
T = df_modified.to_numpy()

new_data = SMOTE(T, N = 1e4, k = 2)
new_data.shape

df_new = pd.DataFrame(data = new_data,
                      index = [i for i in range(new_data.shape[0])],
                      columns = col)
df_new.head()

# After increasing dataset
ax = plt.gca()
df_new.plot(kind='line', y='x100', ax=ax)
df_new.plot(kind='line', y='y100', color='red', ax=ax)
df_new.plot(kind='line', y='z100', color='green', ax=ax)

plt.show()

# Other transformers also can be used!
transformer = Normalizer().fit(df_new)
# Normalize all values between 0 and 0.5 as NN works better w/ them!
array_norm = transformer.transform(df_new)*10
df_norm = pd.DataFrame(data = array_norm,
                      index = [i for i in range(new_data.shape[0])],
                      columns = col)

# Add timeStep column back
df_norm.insert(0, "TimeStep", range(1,len(df_norm)+1))
#df_norm.to_csv("normal_input_trj.csv")
df_norm.head()

ax = plt.gca()
df_norm.plot(kind='line', y='x100', ax=ax)
df_norm.plot(kind='line', y='y100', color='red', ax=ax)
df_norm.plot(kind='line', y='z100', color='green', ax=ax)

plt.show()

sns.jointplot("TimeStep", "z100", df_norm, kind="kde", space=0, color="g")

target = ['x100', 'y100', 'z100'] # Atom 100
shift_steps = 400      # Lag
df_targets = df_norm[target].shift(-shift_steps)
n1 = df_norm.columns.get_loc("x100")

# Use 10 neighbour atoms (33 col in total) as the feature vectors
feature_col = df_norm.columns[n1-15:n1+18]
df_feature = df_norm[feature_col]

x_data = df_feature.values[0:-shift_steps]
y_data = df_targets.values[0:-shift_steps]  

num_data = len(x_data)  
train_split = 0.85       # fraction of the dataset for the training

num_train = int(train_split * num_data)
num_test = num_data - num_train

x_train = x_data[0:num_train]
x_test = x_data[num_train:]

y_train = y_data[0:num_train]
y_test = y_data[num_train:]

num_x_signals = x_data.shape[1]
num_y_signals = y_data.shape[1]

validation_data = (np.expand_dims(x_test, axis=0),
                   np.expand_dims(y_test, axis=0))

print("num_x_signals: ", num_x_signals)

# Batch generator
def batch_generator(batch_size):
    """
    Generator function for creating random batches of training data.
    """
    batch_num = int(num_train/batch_size) + 1
    # Infinite loop
    while True:
        # Allocate a new array for the batch of inputs.
        x_shape = (batch_num, batch_size, num_x_signals)
        x_batch = np.zeros(shape = x_shape, dtype = np.float64)
        print(x_shape)
        # Allocate a new array for the batch of outputs.
        y_shape = (batch_num, batch_size, num_y_signals)
        y_batch = np.zeros(shape = y_shape, dtype = np.float64)

        # Fill the batch with random sequences of data.
        for i in range(batch_num):
            # Get a random start-index.
            # This points somewhere into the training data.
            idx = np.random.randint(num_train - batch_size)
            idx = np.random.randint(num_train - batch_size)
            
            x_batch[i] = x_train[idx:idx+batch_size]
            y_batch[i] = y_train[idx:idx+batch_size]
          
        yield (x_batch, y_batch)

batch_size = 128  # 2^n like!
generator = batch_generator(batch_size = batch_size)
x_batch, y_batch = next(generator)

# Create the Recurrent Neural Network
model = Sequential()
model.add(GRU(units = 256,   
              return_sequences = True,
              input_shape = (None, num_x_signals,)))

# To predict 3 output signals, we add a dense layer,
# which maps output values down to only 3 values.
model.add(Dense(num_y_signals, activation = 'sigmoid'))

if False:
    from tensorflow.keras.initializers import RandomUniform

    # Maybe use lower init-ranges.
    init = RandomUniform(minval=-0.02, maxval=0.02)  
    model.add(Dense(num_y_signals,
                    activation = 'linear',
                    kernel_initializer = init))

# Loss Function: MSE
warmup_steps = 500   

def loss_mse_warmup(y_true, y_pred):
    """
    Calculate the Mean Squared Error between y_true and y_pred,
    but ignore the beginning "warmup" part of the sequences.
    
    y_true is the desired output.
    y_pred is the model's output.
    """

    # The shape of both input tensors are: [batch_num, batch_size, num_x_signals]
    # Ignore the "warmup" parts of the sequences

    y_true_slice = y_true[:, warmup_steps:, :]
    y_pred_slice = y_pred[:, warmup_steps:, :]

    # Calculate the MSE loss for each value in these tensors.
    # Be aware of upgraded version of TensorFlow in colab!
    loss = tf.compat.v1.losses.mean_squared_error(y_true_slice, y_pred_slice)
    loss_mean = tf.reduce_mean(input_tensor = loss)

    return loss_mean

# Compile Model
optimizer = RMSprop(lr = 1e-3)   
model.compile(loss = loss_mse_warmup, optimizer = optimizer, metrics = ['accuracy'])
model.summary()

# Callback Functions
path_checkpoint = '23_checkpoint.keras'
callback_checkpoint = ModelCheckpoint(filepath = path_checkpoint,
                                      monitor = 'val_loss',
                                      verbose = 1,
                                      save_weights_only = True,
                                      save_best_only = True)

callback_early_stopping = EarlyStopping(monitor = 'val_loss',
                                        patience = 8, verbose=1)

callback_tensorboard = TensorBoard(log_dir='./23_logs/',
                                   histogram_freq = 0,   
                                   write_graph = False)  
# Min learning rate 1e-5
callback_reduce_lr = ReduceLROnPlateau(monitor = 'val_loss',
                                       factor = 0.10,    
                                       min_lr = 1e-5,   
                                       patience = 2,
                                       verbose = 1)

callbacks = [callback_early_stopping,
             callback_checkpoint,
             callback_tensorboard,
             callback_reduce_lr]

# Train the Recurrent Neural Network
model.fit_generator(generator = generator,
                    epochs = 30,             # least 20 - keep it higher just for safty!
                    steps_per_epoch = int(num_train/batch_size),   # batch_size would be a good bet
                    validation_data = validation_data,
                    callbacks = callbacks)

# Load Checkpoint
try:
    model.load_weights(path_checkpoint)
except Exception as error:
    print("Error trying to load checkpoint.")
    print(error)

print("batch_size = ", batch_size)

# Generate Predictions
def plot_comparison(start_idx, length):
    """
    Plot the predicted and true output-signals.
    
    :par start_idx: Start-index for the time-series.
    :par length: Sequence length to process and plot.
    """
    # Entire dataset
    x = df_feature.values[:]
    y_true = df_targets.values[:]
    
    end_idx = start_idx + length
    # Pridict the future: the same amount as "shift" value!
    x = x[start_idx:end_idx]
    y_true = y_true[start_idx:end_idx]
    
    # Input signals for the model.
    x = np.expand_dims(x, axis=0)
    y_pred = model.predict(x)    # GRU model

    # print(y_pred[0].shape)
    # print(y_true.shape)

    for signal in range(len(target)):
        # Get the output-signal predicted by the model.
        signal_pred = y_pred[0][:, signal]
        
        # Get the true output-signal from the data-set.
        signal_true = y_true[:, signal]

        # Make the plotting-canvas bigger.
        plt.figure(figsize=(10,5))
        
        # Plot and compare the two signals.
        plt.plot(signal_true, label='true')
        plt.plot(signal_pred, label='pred')
        
        # Plot grey box for warmup period.
        p = plt.axvspan(0, warmup_steps, facecolor='black', alpha=0.3)
        
        # Plot labels
        plt.ylabel(target[signal])
        plt.legend()
        plt.show()

# Plot
plot_comparison(start_idx = 0, length = 10000)

